{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4098fadc",
   "metadata": {},
   "source": [
    "## 05.2 Spatiotemporal Data 2: Multifile Datasets\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "Often times, it's the case that our spatiotemporal datasets are comprised not of 1 file, but of many. Specific examples where this is true include:\n",
    "\n",
    "1. Time series of remote sensing data, where the data is released on the same geospatial footprint at specified intervals (e.g., Landsat row and path footprints),\n",
    "2. Weather forecasting data where each file may be a forecast for a set lead time into the future from a particular initial starting point, \n",
    "3. Climate projection data where each file may contain a year or even a decade of climate information under some given scenario,\n",
    "4. Output of some kind of geophysical model like an ice sheet transport, hydrologic, or snow accumulation and melt model. \n",
    "\n",
    "We also previously saw that the NetCDF file format conveys a lot of advantages because we can store many different variables in the same file, as long as the variables can be described using one or more of the dimensions and coordinate axes describing the file. A double-edged sword of this flexibility, however, lies in the fact that our files can grow very large, very quickly. As I mentioned in class, __one day (24 hours)__ of raw simulated WRF output for our southern Idaho domain produces a single NetCDF file that is 15 GB in size – or about 5.5 TB for a full year of simulation. This predicament leads us to the following sets of questions:\n",
    "\n",
    "- How can we analyze a dataset that is comprised of multiple files, as if it were a single dataset?\n",
    "- Is there a way to analyze multifile datasets, when the sum total of all of the files in the dataet exceed our RAM for our individual machine?\n",
    "\n",
    "Thankfully, `xArray` provides us an answer to both of the above questions. First some background on the data we'll be using today, and the event we'll be investigating. \n",
    "\n",
    "By late May 2010, the water year in the Boise and Payette River basins was shaping up to be great in terms of water resources. Reservoirs were almost full and there was still significant snow at the higher elevations of many water supply basins. However, in early June, a late atmospheric river (AR) would dump significant rain over southwest Idaho, melting a lot of snow in the upper basins and causing rivers to flood. Check out the summary of this event, on the Boise NWS website: [https://www.weather.gov/safety/flood-states-id](https://www.weather.gov/safety/flood-states-id). \n",
    "\n",
    "Many historical weather and climate datasets are not sufficiently fine in spatial or temporal resolutions to be able to analyze the hour-to-hour evolution of this event. As such, we have trouble asking and answering questions like \"how did the rain-snow transition evolve over the course of this storm?\" or \"how did peak-hourly precipitation during the storm vary in space?\" Today, you will be an early adopter of NASA's North American Land Data Assimilation, version 3 (NLDAS-3) data, a new dataset that provides estimates of hydrometeorological variables at 1 km, 1 hr spatiotemporal resolution over all of North America by combining physics based models with available remote sensing data. Read more about NLDAS-3 data here: [https://ldas.gsfc.nasa.gov/nldas/v3](https://ldas.gsfc.nasa.gov/nldas/v3)\n",
    "\n",
    "This notebook uses NLDAS-3 hydrometeorological forcing data to analyze climate conditions in May and June 2010 and assess what fraction of precipitation that fell during those months was associated with this lone AR event. \n",
    "\n",
    "### 2. Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "445b7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Needed for numerical operations\n",
    "import matplotlib.pyplot as plt # Needed for plotting\n",
    "import xarray as xr # Needed for handling NetCDF data\n",
    "import cartopy.crs as ccrs # Needed for cartographic projections\n",
    "import cartopy.feature as cfeature # Needed for cartographic features\n",
    "\n",
    "# Output file parameters\n",
    "nldas3_dir = '../data/ubrb_nldas3_data/' # Directory for NLDAS-3 data\n",
    "nldas3_filebase = 'nldas3_UBRB_subset_' # Base name for NLDAS-3 files\n",
    "nldas3_fileext = '.nc' # File extension for NLDAS-3 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469ecf3",
   "metadata": {},
   "source": [
    "### 3. Load the Data\n",
    "\n",
    "Amazingly, `xArray` can load a multiple-file dataset with a single line of code. We just need to make sure we tell `xArray` how the multiple files in our dataset should be combined. Let's examine what the \"loaded\" dataset looks like and talk about a few things that distinguish the dataset from the underlying files. Specifically, try to find something called \"chunksize\" and see if you can surmise what it might mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef92d95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "chunk manager 'dask' is not available. Please make sure 'dask' is installed and importable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m nldas3_ubrb_ds = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnldas3_dir\u001b[49m\u001b[43m+\u001b[49m\u001b[43mnldas3_filebase\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43mnldas3_fileext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mby_coords\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m nldas3_ubrb_ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/geos505/lib/python3.12/site-packages/xarray/backends/api.py:1624\u001b[39m, in \u001b[36mopen_mfdataset\u001b[39m\u001b[34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, errors, **kwargs)\u001b[39m\n\u001b[32m   1622\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths1d:\n\u001b[32m   1623\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m         ds = \u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m         datasets.append(ds)\n\u001b[32m   1626\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/geos505/lib/python3.12/site-packages/xarray/backends/api.py:602\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, create_default_indexes, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    595\u001b[39m overwrite_encoded_chunks = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33moverwrite_encoded_chunks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    596\u001b[39m backend_ds = backend.open_dataset(\n\u001b[32m    597\u001b[39m     filename_or_obj,\n\u001b[32m    598\u001b[39m     drop_variables=drop_variables,\n\u001b[32m    599\u001b[39m     **decoders,\n\u001b[32m    600\u001b[39m     **kwargs,\n\u001b[32m    601\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m ds = \u001b[43m_dataset_from_backend_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite_encoded_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43minline_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_default_indexes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_default_indexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/geos505/lib/python3.12/site-packages/xarray/backends/api.py:297\u001b[39m, in \u001b[36m_dataset_from_backend_dataset\u001b[39m\u001b[34m(backend_ds, filename_or_obj, engine, chunks, cache, overwrite_encoded_chunks, inline_array, chunked_array_type, from_array_kwargs, create_default_indexes, **extra_tokens)\u001b[39m\n\u001b[32m    294\u001b[39m     ds = backend_ds\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     ds = \u001b[43m_chunk_ds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverwrite_encoded_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43minline_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m ds.set_close(backend_ds._close)\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# Ensure source filename always stored in dataset object\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/geos505/lib/python3.12/site-packages/xarray/backends/api.py:231\u001b[39m, in \u001b[36m_chunk_ds\u001b[39m\u001b[34m(backend_ds, filename_or_obj, engine, chunks, overwrite_encoded_chunks, inline_array, chunked_array_type, from_array_kwargs, **extra_tokens)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chunk_ds\u001b[39m(\n\u001b[32m    221\u001b[39m     backend_ds,\n\u001b[32m    222\u001b[39m     filename_or_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m    229\u001b[39m     **extra_tokens,\n\u001b[32m    230\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     chunkmanager = \u001b[43mguess_chunkmanager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     \u001b[38;5;66;03m# TODO refactor to move this dask-specific logic inside the DaskManager class\u001b[39;00m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunkmanager, DaskManager):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/geos505/lib/python3.12/site-packages/xarray/namedarray/parallelcompat.py:116\u001b[39m, in \u001b[36mguess_chunkmanager\u001b[39m\u001b[34m(manager)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(manager, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_chunkmanagers \u001b[38;5;129;01mand\u001b[39;00m manager \u001b[38;5;129;01min\u001b[39;00m KNOWN_CHUNKMANAGERS:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    117\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchunk manager \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanager\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m is not available.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Please make sure \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mKNOWN_CHUNKMANAGERS[manager]\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m is installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m and importable.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m         )\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(available_chunkmanagers) == \u001b[32m0\u001b[39m:\n\u001b[32m    122\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    123\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mno chunk managers available. Try installing `dask` or another package\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m that provides a chunk manager.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m         )\n",
      "\u001b[31mImportError\u001b[39m: chunk manager 'dask' is not available. Please make sure 'dask' is installed and importable."
     ]
    }
   ],
   "source": [
    "nldas3_ubrb_ds = xr.open_mfdataset(nldas3_dir+nldas3_filebase+'*'+nldas3_fileext, combine='by_coords')\n",
    "nldas3_ubrb_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e7016",
   "metadata": {},
   "source": [
    "### 4. Plot the Total Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f801b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nldas3_ubrb_ds['Rainf'].sum(dim='time').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d040e66",
   "metadata": {},
   "source": [
    "### 5. Isolate AR and non-AR Periods for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_ds = nldas3_ubrb_ds.sel(time=slice('2010-06-02','2010-06-04'))\n",
    "ar_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cf642",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_ds['Rainf'].sum(dim='time').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40012b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_before_ds = nldas3_ubrb_ds.sel(time=slice('2010-05-01','2010-06-01'))\n",
    "ar_after_ds = nldas3_ubrb_ds.sel(time=slice('2010-06-05','2010-06-30'))\n",
    "\n",
    "prcp_no_ar = ar_before_ds['Rainf'].sum(dim='time') + ar_after_ds['Rainf'].sum(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prcp_no_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_ar = ar_ds['Rainf'].sum(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fa648",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_frac = prcp_ar / prcp_no_ar\n",
    "\n",
    "ar_frac.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722419a0",
   "metadata": {},
   "source": [
    "### 6. Create Maps with Spatial Context Using `Cartopy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = ccrs.PlateCarree()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = cfeature.NaturalEarthFeature(\n",
    "    category='cultural'\n",
    "    name='admin_1_states_provinces_lines'\n",
    "    scale='50m'\n",
    "    facecolor='none'\n",
    ")\n",
    "\n",
    "rivers = cfeature.NaturalEarthFeature(\n",
    "    category='physical'\n",
    "    name='rivers_lake_centerlines'\n",
    "    scale='50m'\n",
    "    facecolor='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efcfc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,9),subplot_kw={'projection': projection})\n",
    "\n",
    "# add features\n",
    "ax.add_feature(states, edgecolor='black', linewidth=0.7)\n",
    "ax.add_feature(rivers, edgecolor='blue', linewidth=0.7)\n",
    "\n",
    "im = ax. contourf(ar_frac['lon'],ar_frac['lat'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geos505",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
